##의사결정 트리
	Decision Block(의사결정 블록, 사각형)
	Terminal Block(단말 블록, 타원형)
	Branch(가지)

#장점
	*적은 계산 비용
		-kNN은 많이 계산 비용
	*이해하기 쉬운 학습 결과
		-flowchart로 그려저서 이해하기 쉬움
	*누락된 값 있어도 처리 가능
		-kNN은 그 값을 채워주어야 함
	*분류와 무관한 특징도 처리 가능
		-대표적으로 회귀에 쓰일 수 있음
#단점
	*과적합(overfitting)되기 쉬움,너무 복잡한 의사결정 트리
		-학습데이터에 너무 맞아 떨어진 구조라 과적합이 되기 쉽다.

#학습(의사결정 트리에서): 결정 트리 학습법
	트리 형태로 데이터를 구축하는 것이 의사결정 트리에서의 학습이다.

#의사결정 트리에는 여러가지 방법이 있다. 우리는 그 중에서 ID3를 배운다.

#준비
	*명목형 값은 그냥 사용하면 되지만
	 연속형 값은 구간으로 분리하여 양자화 시켜야 한다.

#가장 적합한 분할 기준을 선택하는 방법
	*정보 이득
	*지니 불순도
	*분산 감소

#정보 이득(Information gain)
	*데이터를 분할하기 이전과 이후의 정보량(엔트로피) 변화
	*정보 이득이 가장 큰 특징에 대해 분할 수행
	*정보 이득으로 정보의 불확실성(엔트로피) 감소

#엔트로피는 정보에 대한 기대값이다.
	엔트로피가 높을수록 정보는 무질서 또는 불확실하다고 할 수 있다.
	확률에 정보량을 곱하는 것이 엔트로피이다.
	정보량이라는 것은 (나는 필요한 정보량이라고 생각) 

#정보 이득(다시)
	정보 이득은 부모 엔트로피에서 분류 후 자식 엔트로피의 값을 뺀 것이다.
	위에서 말했듯이 엔트로피가 낮을수록 정보량은 확실한 것이고 질서가 있는 것이다.
	따라서 자식 엔트로피의 값이 낮을수록 분류가 잘 된 것이므로
	부모 엔트로피-자식 엔트로피의 값이 높을수록 잘 분류된 것이라고 할 수 있다.

## 나이브 베이스: 확률 이론으로 분류하기
	어떤 데이터를 분류할 때, 확률로서 분류한다.
#장점
	소량의 데이터를 가지고도 어느정도 성능이 나온다.
	여러개의 분류 항목을 다룰 수 있다.
#단점
	입력 데이터를 어떻게 준비하느냐에 따라 민감하게 적용

#조건부 확률
	이 이론에서 기본이 되는 이론
#용처
	메일을 분류할 때, 메일의 내용을 분석을 한다. 단어별로 분석을 해서
	이 메일일 때는 이 단어가 몇 퍼센트로 나왔고 저 메일일 때는 이 단어가 몇 퍼센트로 나왔다고
	할때, 어떤 메일의 내용을 분석해서 그것이 스펨인지 아닌지 분류한다.

#현실과는 다른 두 가정
	속성들이 발생할 확률이 서로 독립적이다. 즉 서로에게 영향이 없다.
	모든 속성의 중요도가 같다.

#확률을 구하는 것이 학습의 단계에 해당된다.


























